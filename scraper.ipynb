{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Scraping directory page --------------------\n",
      "-------------------- Found 12 faculty profile urls --------------------\n",
      "-------------------- Scraping faculty url 1/12 --------------------\n",
      "-------------------- Scraping faculty url 2/12 --------------------\n",
      "-------------------- Scraping faculty url 3/12 --------------------\n",
      "-------------------- Scraping faculty url 4/12 --------------------\n",
      "-------------------- Scraping faculty url 5/12 --------------------\n",
      "-------------------- Scraping faculty url 6/12 --------------------\n",
      "-------------------- Scraping faculty url 7/12 --------------------\n",
      "-------------------- Scraping faculty url 8/12 --------------------\n",
      "-------------------- Scraping faculty url 9/12 --------------------\n",
      "-------------------- Scraping faculty url 10/12 --------------------\n",
      "-------------------- Scraping faculty url 11/12 --------------------\n",
      "-------------------- Scraping faculty url 12/12 --------------------\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import re \n",
    "import urllib\n",
    "import time\n",
    "\n",
    "#create a webdriver object and set options for headless browsing\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome('./chromedriver',options=options)\n",
    "\n",
    "#uses webdriver object to execute javascript code and get dynamically loaded webcontent\n",
    "def get_js_soup(url,driver):\n",
    "    driver.get(url)\n",
    "    res_html = driver.execute_script('return document.body.innerHTML')\n",
    "    soup = BeautifulSoup(res_html,'html.parser') #beautiful soup object to be used for parsing html content\n",
    "    return soup\n",
    "\n",
    "#tidies extracted text \n",
    "def process_bio(bio):\n",
    "    bio = bio.encode('ascii',errors='ignore').decode('utf-8')       #removes non-ascii characters\n",
    "    bio = re.sub('\\s+',' ',bio)       #repalces repeated whitespace characters with single space\n",
    "    return bio\n",
    "\n",
    "''' More tidying\n",
    "Sometimes the text extracted HTML webpage may contain javascript code and some style elements. \n",
    "This function removes script and style tags from HTML so that extracted text does not contain them.\n",
    "'''\n",
    "def remove_script(soup):\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    return soup\n",
    "\n",
    "\n",
    "#Checks if bio_url is a valid faculty homepage\n",
    "def is_valid_homepage(bio_url,dir_url):\n",
    "    if bio_url.endswith('.pdf'): #we're not parsing pdfs\n",
    "        return False\n",
    "    try:\n",
    "        #sometimes the homepage url points to the same page as the faculty profile page\n",
    "        #which should be treated differently from an actual homepage\n",
    "        ret_url = urllib.request.urlopen(bio_url).geturl() \n",
    "    except:\n",
    "        return False       #unable to access bio_url\n",
    "    urls = [re.sub('((https?://)|(www.))','',url) for url in [ret_url,dir_url]] #removes url scheme (https,http or www) \n",
    "    return not(urls[0]== urls[1])\n",
    "\n",
    "#extracts all Faculty Profile page urls from the Directory Listing Page\n",
    "def scrape_dir_page(dir_url,driver):\n",
    "    print ('-'*20,'Scraping directory page','-'*20)\n",
    "    faculty_links = []\n",
    "    faculty_base_url = 'https://www.cmu.edu/epp'\n",
    "    #execute js on webpage to load faculty listings on webpage and get ready to parse the loaded HTML \n",
    "    soup = get_js_soup(dir_url,driver)     \n",
    "    for link_holder in soup.find_all('div',class_='photo filterable'): #get list of all <div> of class 'name'\n",
    "        rel_link = link_holder.find('a')['href'] #get url\n",
    "        #url returned is relative, so we need to add base url\n",
    "        faculty_links.append(faculty_base_url+rel_link) \n",
    "    print ('-'*20,'Found {} faculty profile urls'.format(len(faculty_links)),'-'*20)\n",
    "    return faculty_links\n",
    "\n",
    "dir_url = 'https://www.cmu.edu/epp/people/faculty/' #url of directory listings of CS faculty\n",
    "faculty_links = scrape_dir_page(dir_url,driver)\n",
    "\n",
    "def scrape_faculty_page(fac_url,driver):\n",
    "    soup = get_js_soup(fac_url,driver)\n",
    "    homepage_found = False\n",
    "    bio_url = ''\n",
    "    bio = ''\n",
    "    profile_sec = soup.find(class_='content')\n",
    "    if profile_sec is not None:\n",
    "        all_headers = profile_sec.find_all('h1')\n",
    "        faculty_last_name = all_headers[0].get_text().lower().split()[-1] #find faculty last name\n",
    "        faculty_first_name = all_headers[0].get_text().lower().split()[0]\n",
    "        homepage_txts = ['site','page',faculty_last_name,faculty_first_name]\n",
    "        exceptions = ['course ','research','group','cs','mirror','google scholar']\n",
    "        #find the homepage url and extract all text from it\n",
    "        for hdr in all_headers:  #first find the required header\n",
    "            the_website = soup.find(class_ = 'sidebar')\n",
    "            if hdr.text.lower() == 'Website':\n",
    "                next_tag = hdr.find_next('li')\n",
    "                #find <li> which has homepage url\n",
    "                while next_tag is not None: \n",
    "                    cand = next_tag.find('a')\n",
    "                    next_tag = next_tag.next_sibling  #sibling means element present at the same level\n",
    "                    try:\n",
    "                        cand['href']\n",
    "                    except:\n",
    "                        continue\n",
    "                    cand_text = cand.string\n",
    "\n",
    "                    if cand_text is not None and (any(hp_txt in cand_text.lower() for hp_txt in homepage_txts) and \n",
    "                        not any(e in cand_text.lower() for e in exceptions)): #compare text to predefined patterns\n",
    "                        bio_url = cand['href'] \n",
    "                        homepage_found = True\n",
    "                        #check if homepage url is valid\n",
    "                        if not(is_valid_homepage(bio_url,fac_url)):\n",
    "                            homepage_found = False\n",
    "                        else:\n",
    "                            try:\n",
    "                                bio_soup = remove_script(get_js_soup(bio_url,driver)) \n",
    "                            except:\n",
    "                                print ('Could not access {}'.format(bio_url))\n",
    "                                homepage_found = False\n",
    "                        break \n",
    "                if homepage_found:\n",
    "                    #get all the text from homepage(bio)\n",
    "                    bio = process_bio(bio_soup.get_text(separator=' ')) \n",
    "\n",
    "\n",
    "        if not homepage_found:\n",
    "            bio_url = fac_url #treat faculty profile page as homepage\n",
    "            bio = process_bio(profile_sec.get_text(separator=' '))\n",
    "\n",
    "    return bio_url,bio\n",
    "\n",
    "#Scrape homepages of all urls\n",
    "bio_urls, bios = [],[]\n",
    "tot_urls = len(faculty_links)\n",
    "for i,link in enumerate(faculty_links):\n",
    "    print ('-'*20,'Scraping faculty url {}/{}'.format(i+1,tot_urls),'-'*20)\n",
    "    bio_url,bio = scrape_faculty_page(link,driver)\n",
    "    if bio.strip()!= '' and bio_url.strip()!='':\n",
    "        bio_urls.append(bio_url.strip())\n",
    "        bios.append(bio)\n",
    "driver.close()\n",
    "\n",
    "def write_lst(lst,file_):\n",
    "    with open(file_,'w') as f:\n",
    "        for l in lst:\n",
    "            f.write(l)\n",
    "            f.write('\\n')\n",
    "            \n",
    "bio_urls_file = 'bio_urls.txt'\n",
    "bios_file = 'bios.txt'\n",
    "write_lst(bio_urls,bio_urls_file)\n",
    "write_lst(bios,bios_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
